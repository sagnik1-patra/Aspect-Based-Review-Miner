{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd564223-9614-4e82-bf76-551da5198904",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NXTWAVE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "usage: ipykernel_launcher.py [-h] [--data DATA] [--text-col TEXT_COL] [--rating-col RATING_COL] [--id-col ID_COL]\n",
      "                             [--category-col CATEGORY_COL] [--label-col LABEL_COL] [--engine {rule,ml}] [--predict-only] [--do-eda]\n",
      "                             [--outdir OUTDIR] [--seed SEED] [--app] [--init-nltk]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\NXTWAVE\\AppData\\Roaming\\jupyter\\runtime\\kernel-53ade885-314a-4fab-96c4-1c735d9afa03.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 2\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import os, re, json, math, argparse, random, string, warnings\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Optional, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Text processing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from bs4 import BeautifulSoup\n",
    "import emoji as emoji_lib\n",
    "\n",
    "# Sentiment toolkits\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# ML\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Viz\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# UI\n",
    "import gradio as gr\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# -------------- Utils --------------\n",
    "URL_RE = re.compile(r\"https?://\\S+|www\\.\\S+\", re.IGNORECASE)\n",
    "HTML_TAG_RE = re.compile(r\"<[^>]+>\")\n",
    "NON_ASCII_RE = re.compile(r\"[^\\x00-\\x7F]+\")\n",
    "PUNCT_TABLE = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def ensure_dir(p: str | Path) -> Path:\n",
    "    p = Path(p)\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "    return p\n",
    "\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    t = text\n",
    "    # HTML unescape & strip tags\n",
    "    t = BeautifulSoup(t, \"html5lib\").get_text(\" \")\n",
    "    # URLs\n",
    "    t = URL_RE.sub(\" \", t)\n",
    "    # emoji -> text alias (optional)\n",
    "    t = emoji_lib.demojize(t, delimiters=(\" \", \" \"))\n",
    "    # lower\n",
    "    t = t.lower()\n",
    "    # remove punctuation\n",
    "    t = t.translate(PUNCT_TABLE)\n",
    "    # remove non-ascii\n",
    "    t = NON_ASCII_RE.sub(\" \", t)\n",
    "    # collapse whitespace\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "    return t\n",
    "\n",
    "\n",
    "def tokenize_and_lemmatize(t: str, stop_words: set[str]) -> List[str]:\n",
    "    toks = [lemmatizer.lemmatize(w) for w in t.split() if w not in stop_words and len(w) > 2]\n",
    "    return toks\n",
    "\n",
    "\n",
    "def rule_sentiment_score(text: str, use_vader=True, use_textblob=True) -> float:\n",
    "    \"\"\"Return compound polarity in [-1,1]. Combines VADER + TextBlob by averaging when both used.\"\"\"\n",
    "    scores = []\n",
    "    if use_vader:\n",
    "        vs = SentimentIntensityAnalyzer()\n",
    "        scores.append(vs.polarity_scores(text).get('compound', 0.0))\n",
    "    if use_textblob:\n",
    "        try:\n",
    "            scores.append(TextBlob(text).sentiment.polarity)\n",
    "        except Exception:\n",
    "            pass\n",
    "    if not scores:\n",
    "        return 0.0\n",
    "    return float(np.mean(scores))\n",
    "\n",
    "\n",
    "def score_to_label(score: float, pos=0.2, neg=-0.2) -> str:\n",
    "    if score >= pos:\n",
    "        return \"positive\"\n",
    "    if score <= neg:\n",
    "        return \"negative\"\n",
    "    return \"neutral\"\n",
    "\n",
    "\n",
    "# -------------- Data Loading --------------\n",
    "\n",
    "def load_dataset(path: str | Path, text_col: str, rating_col: Optional[str] = None,\n",
    "                 id_col: Optional[str] = None, category_col: Optional[str] = None,\n",
    "                 label_col: Optional[str] = None) -> pd.DataFrame:\n",
    "    path = Path(path)\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Data not found: {path}\")\n",
    "    if path.suffix.lower() == \".json\":\n",
    "        df = pd.read_json(path, lines=True)\n",
    "    else:\n",
    "        df = pd.read_csv(path)\n",
    "    # Normalize column names for safer matching\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "    # Minimal schema\n",
    "    req = [text_col]\n",
    "    for c in req:\n",
    "        if c not in df.columns:\n",
    "            raise ValueError(f\"Column '{c}' not found. Available: {df.columns.tolist()}\")\n",
    "\n",
    "    keep = {text_col: 'reviewText'}\n",
    "    if rating_col and rating_col in df.columns:\n",
    "        keep[rating_col] = 'rating'\n",
    "    if id_col and id_col in df.columns:\n",
    "        keep[id_col] = 'productID'\n",
    "    if category_col and category_col in df.columns:\n",
    "        keep[category_col] = 'productCategory'\n",
    "    if label_col and label_col in df.columns:\n",
    "        keep[label_col] = 'label'\n",
    "\n",
    "    out = df[list(keep.keys())].rename(columns=keep)\n",
    "    return out\n",
    "\n",
    "\n",
    "# -------------- Training --------------\n",
    "\n",
    "def build_ml_pipeline(max_features=50000, ngram_range=(1,2), C=4.0) -> Pipeline:\n",
    "    return Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer(max_features=max_features, ngram_range=ngram_range)),\n",
    "        (\"clf\", LogisticRegression(C=C, max_iter=200, n_jobs=-1)),\n",
    "    ])\n",
    "\n",
    "\n",
    "def prepare_labels_for_ml(df: pd.DataFrame, rating_as_label: bool = True) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    if 'label' in out.columns and not rating_as_label:\n",
    "        # Expect string labels like 'positive'/'negative'\n",
    "        return out\n",
    "    # Construct label from rating if present else fallback via rule-based score\n",
    "    if 'rating' in out.columns and pd.api.types.is_numeric_dtype(out['rating']):\n",
    "        out['label'] = out['rating'].apply(lambda r: 'positive' if r >= 4 else ('negative' if r <= 2 else 'neutral'))\n",
    "    else:\n",
    "        out['label'] = out['reviewText'].fillna(\"\").apply(lambda t: score_to_label(rule_sentiment_score(t)))\n",
    "    return out\n",
    "\n",
    "\n",
    "def train_ml(df: pd.DataFrame, outdir: Path, test_size=0.2, seed=42) -> Dict:\n",
    "    df = df.dropna(subset=['reviewText']).copy()\n",
    "    df = prepare_labels_for_ml(df)\n",
    "    # Drop neutral to make a stricter binary classifier if too few neutrals\n",
    "    label_counts = df['label'].value_counts()\n",
    "    if label_counts.get('neutral', 0) < 50:\n",
    "        df = df[df['label'] != 'neutral']\n",
    "\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "        df['reviewText'], df['label'], test_size=test_size, random_state=seed, stratify=df['label'])\n",
    "\n",
    "    pipe = build_ml_pipeline()\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = pipe.predict(X_valid)\n",
    "    y_proba = None\n",
    "    try:\n",
    "        y_proba = pipe.predict_proba(X_valid)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    report = classification_report(y_valid, y_pred, output_dict=True)\n",
    "    cm = confusion_matrix(y_valid, y_pred, labels=sorted(df['label'].unique()))\n",
    "\n",
    "    # Save artifacts\n",
    "    ensure_dir(outdir)\n",
    "    import joblib\n",
    "    joblib.dump(pipe, outdir / 'model_tfidf_lr.joblib')\n",
    "\n",
    "    # Save evals\n",
    "    pd.DataFrame(report).to_csv(outdir / 'eval_classification_report.csv')\n",
    "    pd.DataFrame(cm, index=sorted(df['label'].unique()), columns=sorted(df['label'].unique())).to_csv(outdir / 'eval_confusion_matrix.csv')\n",
    "\n",
    "    return {\n",
    "        'pipe': pipe,\n",
    "        'report': report,\n",
    "        'cm': cm,\n",
    "        'labels': sorted(df['label'].unique()),\n",
    "        'X_valid': X_valid.reset_index(drop=True),\n",
    "        'y_valid': y_valid.reset_index(drop=True),\n",
    "        'y_pred': pd.Series(y_pred),\n",
    "        'y_proba': y_proba\n",
    "    }\n",
    "\n",
    "\n",
    "# -------------- EDA & Visuals --------------\n",
    "\n",
    "def make_distribution_plot(df: pd.DataFrame, outdir: Path):\n",
    "    plt.figure()\n",
    "    ax = sns.countplot(x='label', data=df, order=['negative','neutral','positive'])\n",
    "    ax.set_title('Sentiment distribution')\n",
    "    for c in ax.containers:\n",
    "        ax.bar_label(c)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outdir / 'viz_sentiment_distribution.png', dpi=160)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def make_rating_vs_sentiment_heatmap(df: pd.DataFrame, outdir: Path):\n",
    "    if 'rating' not in df.columns:\n",
    "        return\n",
    "    tmp = df.copy()\n",
    "    tmp['rating'] = pd.to_numeric(tmp['rating'], errors='coerce')\n",
    "    tmp = tmp.dropna(subset=['rating','label'])\n",
    "    pivot = pd.crosstab(tmp['rating'], tmp['label'])\n",
    "    if pivot.empty:\n",
    "        return\n",
    "    plt.figure()\n",
    "    sns.heatmap(pivot, annot=True, fmt='d')\n",
    "    plt.title('Rating Ã— Sentiment')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outdir / 'viz_rating_x_sentiment_heatmap.png', dpi=160)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def make_wordclouds(df: pd.DataFrame, outdir: Path):\n",
    "    texts = {\n",
    "        'positive': ' '.join(df[df['label']=='positive']['reviewText'].astype(str).tolist())[:3_000_000],\n",
    "        'negative': ' '.join(df[df['label']=='negative']['reviewText'].astype(str).tolist())[:3_000_000],\n",
    "    }\n",
    "    for lab, txt in texts.items():\n",
    "        if not txt:\n",
    "            continue\n",
    "        wc = WordCloud(width=1200, height=600, background_color='white').generate(txt)\n",
    "        wc.to_file(str(outdir / f'viz_wordcloud_{lab}.png'))\n",
    "\n",
    "\n",
    "# --- NEW: Accuracy/F1 graphs and Confusion Matrix heatmaps ---\n",
    "\n",
    "def plot_accuracy_graph(report: Dict, outdir: Path):\n",
    "    \"\"\"Save a simple accuracy/f1 bar chart: overall accuracy + macro/weighted F1.\"\"\"\n",
    "    acc = float(report.get('accuracy', 0.0)) if isinstance(report, dict) else 0.0\n",
    "    macro_f1 = float(report.get('macro avg', {}).get('f1-score', 0.0)) if isinstance(report, dict) else 0.0\n",
    "    weighted_f1 = float(report.get('weighted avg', {}).get('f1-score', 0.0)) if isinstance(report, dict) else 0.0\n",
    "    plt.figure()\n",
    "    ax = sns.barplot(x=['accuracy','macro_f1','weighted_f1'], y=[acc, macro_f1, weighted_f1])\n",
    "    ax.set_ylim(0,1)\n",
    "    ax.set_title('Model accuracy & F1 scores')\n",
    "    for c in ax.containers:\n",
    "        ax.bar_label(c, fmt='%.3f')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outdir / 'viz_accuracy.png', dpi=160)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_prf1_by_class(report: Dict, outdir: Path):\n",
    "    \"\"\"Bar chart of precision/recall/F1 per class label.\"\"\"\n",
    "    # Extract class rows (exclude 'accuracy', 'macro avg', 'weighted avg')\n",
    "    class_rows = {k:v for k,v in report.items() if isinstance(v, dict) and k not in ('accuracy','macro avg','weighted avg')}\n",
    "    if not class_rows:\n",
    "        return\n",
    "    labels = list(class_rows.keys())\n",
    "    prec = [class_rows[k].get('precision', 0.0) for k in labels]\n",
    "    rec  = [class_rows[k].get('recall', 0.0) for k in labels]\n",
    "    f1   = [class_rows[k].get('f1-score', 0.0) for k in labels]\n",
    "    x = np.arange(len(labels))\n",
    "    w = 0.25\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.bar(x - w, prec, width=w, label='precision')\n",
    "    plt.bar(x,     rec,  width=w, label='recall')\n",
    "    plt.bar(x + w, f1,   width=w, label='f1')\n",
    "    plt.xticks(x, labels)\n",
    "    plt.ylim(0,1)\n",
    "    plt.title('Perâ€‘class precision/recall/F1')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outdir / 'viz_prf1_by_class.png', dpi=160)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "essentially_zero = 1e-12\n",
    "\n",
    "def plot_confusion_heatmaps(cm: np.ndarray, labels: List[str], outdir: Path):\n",
    "    \"\"\"Save confusion matrix (counts) and rowâ€‘normalized heatmaps.\"\"\"\n",
    "    if cm is None or len(cm)==0:\n",
    "        return\n",
    "    # Counts\n",
    "    plt.figure()\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix (counts)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outdir / 'viz_confusion_matrix.png', dpi=160)\n",
    "    plt.close()\n",
    "    # Rowâ€‘normalized\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        row_sums = cm.sum(axis=1, keepdims=True)\n",
    "        norm = np.divide(cm, np.where(row_sums==0, 1, row_sums))\n",
    "    plt.figure()\n",
    "    sns.heatmap(norm, annot=True, fmt='.2f', cmap='Greens', xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix (rowâ€‘normalized)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outdir / 'viz_confusion_matrix_normalized.png', dpi=160)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def top_keywords(df: pd.DataFrame, label: str, k: int = 20) -> List[Tuple[str,int]]:\n",
    "    stop = set(stopwords.words('english'))\n",
    "    words = []\n",
    "    for t in df[df['label']==label]['reviewText'].dropna():\n",
    "        t = clean_text(t)\n",
    "        words.extend([w for w in t.split() if w not in stop and len(w)>2 and not w.isdigit()])\n",
    "    vc = pd.Series(words).value_counts().head(k)\n",
    "    return list(vc.items())\n",
    "\n",
    "\n",
    "def insight_products_with_hidden_issues(df: pd.DataFrame, rating_threshold=4) -> pd.DataFrame:\n",
    "    # High average rating but with meaningful fraction of negative labels\n",
    "    if 'productID' not in df.columns:\n",
    "        return pd.DataFrame()\n",
    "    agg = df.groupby('productID').agg(\n",
    "        avg_rating=('rating','mean'),\n",
    "        n_reviews=('reviewText','count'),\n",
    "        neg_frac=('label', lambda s: (s=='negative').mean())\n",
    "    ).reset_index()\n",
    "    out = agg[(agg['avg_rating']>=rating_threshold) & (agg['neg_frac']>=0.15) & (agg['n_reviews']>=10)]\n",
    "    return out.sort_values(['neg_frac','n_reviews'], ascending=[False,False]).head(20)\n",
    "\n",
    "\n",
    "# -------------- Prediction helpers --------------\n",
    "\n",
    "def predict_rule(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    vs = SentimentIntensityAnalyzer()\n",
    "    rows = []\n",
    "    for t in tqdm(df['reviewText'].fillna(\"\"), desc='Rule sentiment'):\n",
    "        comp = vs.polarity_scores(t)['compound']\n",
    "        lab = score_to_label(comp)\n",
    "        rows.append((comp, lab))\n",
    "    out = df.copy()\n",
    "    out['score'] = [r[0] for r in rows]\n",
    "    out['label'] = [r[1] for r in rows]\n",
    "    return out\n",
    "\n",
    "\n",
    "def predict_ml(pipe: Pipeline, texts: List[str]) -> Tuple[List[str], Optional[np.ndarray]]:\n",
    "    try:\n",
    "        proba = pipe.predict_proba(texts)\n",
    "        idx = np.argmax(proba, axis=1)\n",
    "        labels = pipe.classes_[idx]\n",
    "        return labels.tolist(), proba\n",
    "    except Exception:\n",
    "        labels = pipe.predict(texts)\n",
    "        return labels.tolist(), None\n",
    "\n",
    "\n",
    "# -------------- Gradio UI --------------\n",
    "\n",
    "def launch_app(outdir: Path):\n",
    "    import joblib\n",
    "    pipe = None\n",
    "    model_path = outdir / 'model_tfidf_lr.joblib'\n",
    "    if model_path.exists():\n",
    "        pipe = joblib.load(model_path)\n",
    "\n",
    "    def _predict_one(text):\n",
    "        if not text or not text.strip():\n",
    "            return \"\", 0.0\n",
    "        if pipe is None:\n",
    "            comp = rule_sentiment_score(text)\n",
    "            return score_to_label(comp), float(comp)\n",
    "        else:\n",
    "            lab, proba = predict_ml(pipe, [text])\n",
    "            conf = 0.0\n",
    "            if proba is not None:\n",
    "                conf = float(np.max(proba))\n",
    "            return lab[0], conf\n",
    "\n",
    "    def _predict_batch(texts):\n",
    "        items = [t.strip() for t in texts.split('\\n') if t.strip()]\n",
    "        if not items:\n",
    "            return \"\"\n",
    "        out = []\n",
    "        for t in items:\n",
    "            lab, score = _predict_one(t)\n",
    "            out.append({\"text\": t, \"sentiment\": lab, \"confidence\": score})\n",
    "        return pd.DataFrame(out)\n",
    "\n",
    "    with gr.Blocks(title=\"ReviewSentiment Analyzer\") as demo:\n",
    "        gr.Markdown(\"# ðŸ›’ ReviewSentiment Analyzer\\nEnter a review to classify sentiment.\")\n",
    "        with gr.Tab(\"Single\"):\n",
    "            inp = gr.Textbox(label=\"Review text\", lines=4)\n",
    "            btn = gr.Button(\"Predict\")\n",
    "            out_lab = gr.Label(num_top_classes=3, label=\"Sentiment (top)\"\n",
    "                               ) if pipe is not None else gr.Textbox(label=\"Sentiment\")\n",
    "            out_conf = gr.Number(label=\"Confidence / Score\")\n",
    "            btn.click(_predict_one, inputs=inp, outputs=[out_lab, out_conf])\n",
    "        with gr.Tab(\"Batch\"):\n",
    "            tb = gr.Textbox(label=\"One review per line\", lines=8)\n",
    "            btnb = gr.Button(\"Predict batch\")\n",
    "            outdf = gr.Dataframe(interactive=False)\n",
    "            btnb.click(_predict_batch, inputs=tb, outputs=outdf)\n",
    "        gr.Markdown(\"Model: \" + (\"TFâ€‘IDF + LogisticRegression\" if pipe is not None else \"Ruleâ€‘based (VADER/TextBlob)\"))\n",
    "    demo.launch(server_name=\"0.0.0.0\", server_port=7860, show_error=True)\n",
    "\n",
    "\n",
    "# -------------- Main CLI --------------\n",
    "\n",
    "def init_nltk():\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "\n",
    "\n",
    "def do_eda(df: pd.DataFrame, outdir: Path):\n",
    "    make_distribution_plot(df, outdir)\n",
    "    make_rating_vs_sentiment_heatmap(df, outdir)\n",
    "    make_wordclouds(df, outdir)\n",
    "\n",
    "    # Keyword tables\n",
    "    pos_kw = top_keywords(df, 'positive', 25)\n",
    "    neg_kw = top_keywords(df, 'negative', 25)\n",
    "    pd.DataFrame(pos_kw, columns=['keyword','count']).to_csv(outdir / 'top_keywords_positive.csv', index=False)\n",
    "    pd.DataFrame(neg_kw, columns=['keyword','count']).to_csv(outdir / 'top_keywords_negative.csv', index=False)\n",
    "\n",
    "    # Hidden issues\n",
    "    hidden = insight_products_with_hidden_issues(df)\n",
    "    if not hidden.empty:\n",
    "        hidden.to_csv(outdir / 'products_hidden_issues.csv', index=False)\n",
    "\n",
    "\n",
    "def main():\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument('--data', type=str, help='Path to CSV/JSON with reviews')\n",
    "    ap.add_argument('--text-col', type=str, default='reviewText')\n",
    "    ap.add_argument('--rating-col', type=str, default=None)\n",
    "    ap.add_argument('--id-col', type=str, default=None)\n",
    "    ap.add_argument('--category-col', type=str, default=None)\n",
    "    ap.add_argument('--label-col', type=str, default=None, help='If provided, supervised labels in {positive,negative,neutral}')\n",
    "\n",
    "    ap.add_argument('--engine', choices=['rule','ml'], default='ml')\n",
    "    ap.add_argument('--predict-only', action='store_true', help='Skip training; just score with rule engine')\n",
    "    ap.add_argument('--do-eda', action='store_true')\n",
    "    ap.add_argument('--outdir', type=str, default='./sentiment_out')\n",
    "    ap.add_argument('--seed', type=int, default=42)\n",
    "    ap.add_argument('--app', action='store_true', help='Launch Gradio app')\n",
    "    ap.add_argument('--init-nltk', action='store_true')\n",
    "\n",
    "    args = ap.parse_args()\n",
    "\n",
    "    outdir = ensure_dir(args.outdir)\n",
    "\n",
    "    if args.init_nltk:\n",
    "        init_nltk()\n",
    "        print('[OK] NLTK resources downloaded.')\n",
    "        if not args.data and not args.app:\n",
    "            return\n",
    "\n",
    "    if args.app:\n",
    "        launch_app(outdir)\n",
    "        return\n",
    "\n",
    "    if not args.data:\n",
    "        raise SystemExit('Provide --data path to run training/scoring.')\n",
    "\n",
    "    # Load data and map columns\n",
    "    df_raw = load_dataset(\n",
    "        args.data,\n",
    "        text_col=args.text_col,\n",
    "        rating_col=args.rating_col,\n",
    "        id_col=args.id_col,\n",
    "        category_col=args.category_col,\n",
    "        label_col=args.label_col,\n",
    "    )\n",
    "\n",
    "    # Basic cleaning & labeling\n",
    "    print(f\"[INFO] Loaded {len(df_raw)} rows. Columns: {df_raw.columns.tolist()}\")\n",
    "    df = df_raw.copy()\n",
    "    df['reviewText'] = df['reviewText'].astype(str).map(clean_text)\n",
    "\n",
    "    # Sentiment path\n",
    "    if args.engine == 'rule' or args.predict_only:\n",
    "        print('[INFO] Using rule-based sentiment (VADER/TextBlob).')\n",
    "        df_scored = predict_rule(df)\n",
    "        df_scored.to_csv(outdir / 'predictions_rule_based.csv', index=False)\n",
    "        print('[OK] Saved predictions_rule_based.csv')\n",
    "        if args.do_eda:\n",
    "            do_eda(df_scored, outdir)\n",
    "        return\n",
    "\n",
    "    # ML path (train + eval + visuals)\n",
    "    print('[INFO] Training ML model (TFâ€‘IDF + LogisticRegression) ...')\n",
    "    res = train_ml(df, outdir, seed=args.seed)\n",
    "\n",
    "    # NEW: accuracy graph + per-class PR/F1 + confusion heatmaps\n",
    "    try:\n",
    "        plot_accuracy_graph(res['report'], outdir)\n",
    "        plot_prf1_by_class(res['report'], outdir)\n",
    "        plot_confusion_heatmaps(res['cm'], res['labels'], outdir)\n",
    "        print('[OK] Saved accuracy graph and confusion heatmaps')\n",
    "    except Exception as e:\n",
    "        print('[WARN] Could not save accuracy/heatmap visuals:', e)\n",
    "\n",
    "    eval_df = pd.DataFrame({\n",
    "        'text': res['X_valid'],\n",
    "        'label_true': res['y_valid'],\n",
    "        'label_pred': res['y_pred']\n",
    "    })\n",
    "    eval_df.to_csv(outdir / 'eval_predictions_valid.csv', index=False)\n",
    "    print('[OK] Wrote eval artifacts to', outdir)\n",
    "\n",
    "    # EDA on full dataset using model labels for consistency\n",
    "    df_for_eda = df.copy()\n",
    "    # Get labels for whole corpus (might be slow on huge datasets)\n",
    "    print('[INFO] Scoring entire corpus for EDA ...')\n",
    "    labels_all, _ = predict_ml(res['pipe'], df_for_eda['reviewText'].tolist())\n",
    "    df_for_eda['label'] = labels_all\n",
    "\n",
    "    do_eda(df_for_eda, outdir)\n",
    "    print('[OK] Visuals exported to', outdir)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cd70b8-65e5-43b6-809d-fc225ba31c28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
