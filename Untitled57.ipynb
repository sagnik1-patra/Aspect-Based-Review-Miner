{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e16647e-c631-4642-8c83-9a26000f2bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Laptop_Train_v2.csv → C:\\Users\\NXTWAVE\\Downloads\\Aspect Based Review Miner\\predictions_Laptop_Train_v2.csv\n",
      "[OK] Laptops_Test_Data_PhaseA.csv → C:\\Users\\NXTWAVE\\Downloads\\Aspect Based Review Miner\\predictions_Laptops_Test_Data_PhaseA.csv\n",
      "[OK] Laptops_Test_Data_PhaseB.csv → C:\\Users\\NXTWAVE\\Downloads\\Aspect Based Review Miner\\predictions_Laptops_Test_Data_PhaseB.csv\n",
      "[OK] Restaurants_Test_Data_PhaseA.csv → C:\\Users\\NXTWAVE\\Downloads\\Aspect Based Review Miner\\predictions_Restaurants_Test_Data_PhaseA.csv\n",
      "[OK] Restaurants_Train_v2.csv → C:\\Users\\NXTWAVE\\Downloads\\Aspect Based Review Miner\\predictions_Restaurants_Train_v2.csv\n",
      "[OK] restaurants-trial.csv → C:\\Users\\NXTWAVE\\Downloads\\Aspect Based Review Miner\\predictions_restaurants-trial.csv\n",
      "[OK] Combined → C:\\Users\\NXTWAVE\\Downloads\\Aspect Based Review Miner\\predictions_all.csv\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import argparse, json, sys\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import re, string\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# ---------------- Configuration ----------------\n",
    "OUT_DIR   = Path(r\"C:\\Users\\NXTWAVE\\Downloads\\Aspect Based Review Miner\")\n",
    "MODEL_PATH = OUT_DIR / 'model_aspect_sentiment.pkl'\n",
    "DEFAULT_INPUTS = [\n",
    "    r\"C:\\Users\\NXTWAVE\\Downloads\\Aspect Based Review Miner\\archive\\Laptop_Train_v2.csv\",\n",
    "    r\"C:\\Users\\NXTWAVE\\Downloads\\Aspect Based Review Miner\\archive\\Laptops_Test_Data_PhaseA.csv\",\n",
    "    r\"C:\\Users\\NXTWAVE\\Downloads\\Aspect Based Review Miner\\archive\\Laptops_Test_Data_PhaseB.csv\",\n",
    "    r\"C:\\Users\\NXTWAVE\\Downloads\\Aspect Based Review Miner\\archive\\Restaurants_Test_Data_PhaseA.csv\",\n",
    "    r\"C:\\Users\\NXTWAVE\\Downloads\\Aspect Based Review Miner\\archive\\Restaurants_Train_v2.csv\",\n",
    "    r\"C:\\Users\\NXTWAVE\\Downloads\\Aspect Based Review Miner\\archive\\restaurants-trial.csv\",\n",
    "]\n",
    "\n",
    "# Column candidates across ABSA datasets\n",
    "TEXT_CANDS      = [\"text\",\"Text\",\"sentence\",\"Sentence\",\"Review\",\"review\",\"ReviewText\",\"reviewText\"]\n",
    "CATEGORY_CANDS  = [\"category\",\"Category\",\"AspectCategory\",\"aspect\",\"Aspect\",\"target\",\"Target\",\"Term\",\"Aspect Term\",\"Aspect_Term\"]\n",
    "ID_CANDS        = [\"id\",\"ID\",\"SentenceID\",\"sentence_id\",\"ReviewID\",\"review_id\"]\n",
    "\n",
    "# ---------------- Helpers ----------------\n",
    "URL_RE = re.compile(r\"https?://\\S+|www\\.\\S+\", re.IGNORECASE)\n",
    "PUNCT_TABLE = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "def ensure_outdir(p: Path):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def standardize_category(v: str) -> str:\n",
    "    if not isinstance(v, str):\n",
    "        return \"UNCAT\"\n",
    "    t = v.strip().replace(' ', '_').upper()\n",
    "    return t or \"UNCAT\"\n",
    "\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    t = BeautifulSoup(s, \"html5lib\").get_text(\" \")\n",
    "    t = URL_RE.sub(\" \", t).lower().translate(PUNCT_TABLE)\n",
    "    t = re.sub(r\"[^a-z0-9\\s]\", \" \", t)\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "    return t\n",
    "\n",
    "\n",
    "def pick_col(df: pd.DataFrame, cands: List[str]) -> Optional[str]:\n",
    "    for c in cands:\n",
    "        if c in df.columns: return c\n",
    "    return None\n",
    "\n",
    "\n",
    "def normalize_input(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    tcol = pick_col(df, TEXT_CANDS)\n",
    "    if not tcol:\n",
    "        raise ValueError(f\"No text column found in {list(df.columns)}\")\n",
    "    ccol = pick_col(df, CATEGORY_CANDS)\n",
    "    icol = pick_col(df, ID_CANDS)\n",
    "    keep = {tcol: 'text'}\n",
    "    if ccol: keep[ccol] = 'category'\n",
    "    if icol: keep[icol] = 'id'\n",
    "    out = df[list(keep.keys())].rename(columns=keep)\n",
    "    out['text'] = out['text'].astype(str)\n",
    "    if 'category' in out.columns:\n",
    "        out['category'] = out['category'].astype(str).map(standardize_category)\n",
    "    else:\n",
    "        out['category'] = 'UNCAT'\n",
    "    return out\n",
    "\n",
    "\n",
    "def prepare_features(df: pd.DataFrame) -> pd.Series:\n",
    "    tclean = df['text'].map(clean_text)\n",
    "    cat = df['category'].fillna('UNCAT')\n",
    "    feat = '[' + cat + '] ' + tclean\n",
    "    return feat\n",
    "\n",
    "\n",
    "def load_model(path: Path):\n",
    "    if not path.exists():\n",
    "        raise SystemExit(f\"Model not found: {path}\\nRun the builder script first to create model_aspect_sentiment.pkl.\")\n",
    "    return joblib.load(path)\n",
    "\n",
    "\n",
    "def predict_one_file(file_path: Path, model, out_dir: Path) -> Path:\n",
    "    # read CSV with safe fallbacks\n",
    "    try:\n",
    "        df_in = pd.read_csv(file_path)\n",
    "    except Exception:\n",
    "        df_in = pd.read_csv(file_path, encoding='latin-1')\n",
    "\n",
    "    base = file_path.stem\n",
    "    try:\n",
    "        X = normalize_input(df_in)\n",
    "    except Exception as e:\n",
    "        raise SystemExit(f\"{file_path.name}: {e}\")\n",
    "\n",
    "    feats = prepare_features(X)\n",
    "\n",
    "    # Predict labels and probabilities (if available)\n",
    "    try:\n",
    "        proba = model.predict_proba(feats)\n",
    "        idx = np.argmax(proba, axis=1)\n",
    "        yhat = model.classes_[idx]\n",
    "        conf = proba.max(axis=1)\n",
    "    except Exception:\n",
    "        yhat = model.predict(feats)\n",
    "        proba = None\n",
    "        conf = np.zeros(len(yhat))\n",
    "\n",
    "    # Build output frame: original + normalized + predictions\n",
    "    out = df_in.copy()\n",
    "    out['predicted_polarity'] = pd.Series(yhat).astype(str)\n",
    "    out['confidence'] = conf\n",
    "\n",
    "    # include per-class probabilities if available\n",
    "    if proba is not None:\n",
    "        for i, cls in enumerate(model.classes_):\n",
    "            out[f'proba_{cls}'] = proba[:, i]\n",
    "\n",
    "    out_path = out_dir / f\"predictions_{base}.csv\"\n",
    "    out.to_csv(out_path, index=False)\n",
    "\n",
    "    # insights json for this file\n",
    "    counts = pd.Series(yhat).value_counts().to_dict()\n",
    "    insights = {\"file\": str(file_path), \"distribution\": counts}\n",
    "    with open(out_dir / f\"insights_{base}.json\", 'w', encoding='utf-8') as f:\n",
    "        json.dump(insights, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"[OK] {file_path.name} → {out_path}\")\n",
    "    return out_path\n",
    "\n",
    "\n",
    "# ---------------- Main ----------------\n",
    "\n",
    "def main(argv: Optional[List[str]] = None):\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument('--files', nargs='*', help='Paths to CSV files to score')\n",
    "    ap.add_argument('--outdir', type=str, default=str(OUT_DIR))\n",
    "    args, _ = ap.parse_known_args(argv)\n",
    "\n",
    "    outdir = Path(args.outdir)\n",
    "    ensure_outdir(outdir)\n",
    "\n",
    "    model = load_model(MODEL_PATH)\n",
    "\n",
    "    # Resolve file list: explicit or defaults\n",
    "    files: List[Path] = []\n",
    "    if args.files:\n",
    "        files = [Path(p) for p in args.files if Path(p).exists()]\n",
    "    else:\n",
    "        files = [Path(p) for p in DEFAULT_INPUTS if Path(p).exists()]\n",
    "\n",
    "    if not files:\n",
    "        raise SystemExit(\"No input files found. Pass --files paths or place CSVs in the archive folder.\")\n",
    "\n",
    "    outputs = []\n",
    "    for fp in files:\n",
    "        outputs.append(predict_one_file(fp, model, outdir))\n",
    "\n",
    "    # If multiple, also concatenate into predictions_all.csv\n",
    "    if len(outputs) > 1:\n",
    "        frames = []\n",
    "        for op in outputs:\n",
    "            try:\n",
    "                dfp = pd.read_csv(op)\n",
    "                dfp['__source__'] = Path(op).name\n",
    "                frames.append(dfp)\n",
    "            except Exception:\n",
    "                pass\n",
    "        if frames:\n",
    "            all_df = pd.concat(frames, ignore_index=True)\n",
    "            all_path = outdir / 'predictions_all.csv'\n",
    "            all_df.to_csv(all_path, index=False)\n",
    "            print(f\"[OK] Combined → {all_path}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main(sys.argv[1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89997958-26aa-4032-bb45-8677203d0b3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
